# Customer Retention Intelligence System
## Complete Project Structure

```
retention_system/
│
├── README.md                              # Main project README
├── PILLAR_0_README.md                     # Detailed Pillar 0 documentation
├── requirements.txt                       # Python dependencies
│
├── config/
│   └── config.yaml                        # System configuration (ALL hyperparameters)
│
├── src/
│   ├── __init__.py
│   │
│   ├── utils/
│   │   ├── __init__.py
│   │   └── utils.py                       # Configuration, logging, utilities
│   │
│   ├── data/
│   │   ├── __init__.py
│   │   ├── validation.py                  # Data quality validation
│   │   ├── cleaning.py                    # 5-stage cleaning pipeline
│   │   └── feature_engineering.py         # Point-in-time feature creation
│   │
│   └── pillar_0_main.py                   # Main execution script
│
├── examples/
│   └── pillar_0_examples.py               # Usage examples (6 scenarios)
│
├── data/                                   # Created by pipeline
│   ├── raw/                                # Place UCI dataset here
│   │   └── online_retail.csv             # (Download from UCI ML Repository)
│   ├── processed/                          # Generated by cleaning
│   │   └── cleaned_data.parquet
│   └── features/                           # Generated by feature engineering
│       ├── customer_features.parquet
│       └── feature_metadata.json
│
├── outputs/                                # Generated reports
│   ├── validation_report.csv
│   └── cleaning_stats.json
│
├── logs/                                   # Execution logs
│   └── retention_system.log
│
├── models/                                 # For future pillars
│   └── (Pillar 1-4 models will go here)
│
└── tests/                                  # Unit tests (TODO)
    ├── test_validation.py
    ├── test_cleaning.py
    └── test_features.py
```

---

## Quick Start Guide

### Step 1: Download UCI Dataset

```bash
# Download from: https://archive.ics.uci.edu/ml/datasets/Online+Retail
# Or use wget:
mkdir -p data/raw
wget https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx \
  -O data/raw/Online\ Retail.xlsx

# Convert XLSX to CSV (if needed)
# Use Excel or Python: pd.read_excel('Online Retail.xlsx').to_csv('online_retail.csv')
```

### Step 2: Install Dependencies

```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### Step 3: Run Pipeline

```bash
# Full pipeline (all snapshots)
python src/pillar_0_main.py --raw-data data/raw/online_retail.csv

# Fast mode (single snapshot for testing)
python src/pillar_0_main.py --raw-data data/raw/online_retail.csv --single-snapshot
```

### Step 4: Explore Results

```bash
# Run examples
python examples/pillar_0_examples.py --example 1  # Basic usage
python examples/pillar_0_examples.py --example 3  # Inspect features
python examples/pillar_0_examples.py --example 5  # Quality report
```

---

## Module Descriptions

### Configuration (`config/config.yaml`)
**Purpose**: Centralized settings for entire system

**Key Sections**:
- `data_cleaning`: Thresholds, filters, outlier detection
- `feature_engineering`: Time windows, aggregations
- `churn`: Churn window (90 days), observation window (180 days)
- `clv`: Prediction horizon, discount rate
- `paths`: All input/output directories

**Why it matters**: Change parameters in ONE place, not scattered across code

---

### Utilities (`src/utils/utils.py`)
**Purpose**: Common functions used across all modules

**Key Classes/Functions**:
- `Config`: Load and access YAML configuration
- `setup_logging()`: Consistent logging across modules
- `Timer`: Context manager for timing operations
- `save_dataframe()`, `load_dataframe()`: Consistent file I/O
- `print_section_header()`: Formatted console output

**Why it matters**: DRY principle, maintainability

---

### Validation (`src/data/validation.py`)
**Purpose**: Comprehensive data quality checks

**Key Class**: `DataValidator`

**Validation Checks** (8 categories):
1. Schema validation
2. Completeness (missing values)
3. Data type compatibility
4. Value ranges
5. Business rules
6. Duplicate detection
7. Date validation
8. Customer consistency

**Output**: Validation report with pass/fail for each check

**Why it matters**: Catch data issues early, prevent garbage-in-garbage-out

---

### Cleaning (`src/data/cleaning.py`)
**Purpose**: Transform raw messy data into clean, analysis-ready data

**Key Class**: `DataCleaner`

**Pipeline Stages** (5 stages):
1. **Structural**: Remove nulls, parse dates, convert types
2. **Business Logic**: Handle returns, cancellations, invalid transactions
3. **Outliers**: Remove extreme prices and quantities
4. **Customer-Level**: Enforce minimum transactions, flag anomalies
5. **Feature Creation**: Compute Revenue, extract date components

**Output**: Cleaned Parquet file + statistics JSON

**Why it matters**: Quality data = quality models

---

### Feature Engineering (`src/data/feature_engineering.py`)
**Purpose**: Create predictive features with temporal consistency

**Key Class**: `FeatureEngineer`

**Critical Concept**: **Point-in-Time Correctness**
- Only use data available at prediction time
- Prevents data leakage
- Snapshot-based computation

**Feature Groups** (5 groups, ~33 features):
1. **RFM**: Recency, Frequency, Monetary
2. **Temporal**: Transaction patterns, interpurchase times
3. **Product**: Basket size, diversity, return rate
4. **Engagement**: Scores, consistency metrics
5. **Trend**: Growth/decline slopes

**Output**: Feature store with multiple snapshots

**Why it matters**: These features power churn/CLV models in next pillars

---

## Design Philosophy

### 1. Production-First
- Configuration-driven (no hardcoded values)
- Comprehensive logging
- Error handling
- Scalable architecture

### 2. Temporal Consistency
- No data leakage
- Point-in-time correct features
- Snapshot-based validation

### 3. Modular Design
- Each component is independent
- Easy to test
- Easy to extend

### 4. Business-Aware
- Features aligned with retention use case
- Interpretable metrics
- Domain knowledge embedded

---

## Performance Characteristics

### Data Volume Support
- **Small** (<100K rows): 1-2 minutes
- **Medium** (100K-1M rows): 5-10 minutes
- **Large** (1M-10M rows): 30-60 minutes
- **Very Large** (>10M rows): Consider sampling or distributed processing

### Memory Requirements
- **Typical**: 1-2 GB peak
- **Large datasets**: 4-8 GB
- **Optimization**: Use chunking for very large files

### Scalability Strategies
1. **Sampling**: Random sample for exploration
2. **Chunking**: Process data in batches
3. **Parallel Processing**: Multi-core feature computation
4. **Distributed**: Spark/Dask for >10M rows

---

## Next Steps After Pillar 0

### Pillar 1: RFM Segmentation (Unsupervised Learning)
- K-Means clustering on RFM features
- Customer segment profiling
- Segment-based strategies

### Pillar 2: Churn Prediction (Supervised Classification)
- Create churn labels (90-day inactivity)
- Train LightGBM classifier
- Optimize decision threshold
- Evaluate with business metrics

### Pillar 3: CLV Prediction (Regression)
- Predict 180-day revenue
- Compare regression vs BG/NBD
- Quantile regression for uncertainty

### Pillar 4: Revenue Optimization (Decision Optimization)
- Expected value calculation
- Budget-constrained allocation
- Multi-tier campaign design
- ROI maximization

---

## Common Questions

**Q: Why Parquet instead of CSV?**
A: Faster read/write, smaller size, preserves data types, columnar storage for analytics

**Q: Why keep returns and cancellations?**
A: They're important signals for churn prediction and customer behavior analysis

**Q: What's the minimum data requirement?**
A: 270 days (180 observation + 90 prediction window) with >1000 customers

**Q: Can I use this with my company's data?**
A: Yes! Just map your columns to the required schema and adjust config.yaml

**Q: How do I handle missing CustomerID?**
A: These are removed (B2B/guest transactions). If critical, create synthetic IDs

**Q: Why multiple snapshots?**
A: To train models with multiple time periods and validate temporal consistency

---

## Troubleshooting

### Issue: Import errors
**Solution**: Make sure you're in the project root and `src/` is in Python path

### Issue: Out of memory
**Solution**: Use `--single-snapshot` flag or process fewer snapshots

### Issue: Date parsing fails
**Solution**: Update `date_format` in config.yaml to match your data

### Issue: No snapshots generated
**Solution**: Your dataset is too short. Need at least 270 days of data

---

## Code Quality Standards

This codebase follows production standards:
- ✅ Type hints on function signatures
- ✅ Comprehensive docstrings
- ✅ Logging instead of print statements
- ✅ Configuration-driven (no magic numbers)
- ✅ Error handling with try-except
- ✅ Modular, testable design
- ✅ DRY principle (no code duplication)
- ✅ Meaningful variable names
- ⏳ Unit tests (TODO: next iteration)

---

## License & Attribution

**Dataset**: UCI Online Retail Dataset
- Citation: Daqing Chen, Sai Liang Sain, and Kun Guo
- Source: UCI Machine Learning Repository
- License: Public domain for research

**Code**: Original implementation
- For educational and interview purposes
- Feel free to adapt for your use case

---

## Support & Feedback

**Logs**: Check `logs/retention_system.log` for detailed execution trace

**Reports**: Review `outputs/validation_report.csv` for data quality issues

**Configuration**: Adjust `config/config.yaml` for your specific needs

---

**Status**: Pillar 0 Complete - 6/6 Tasks ✓

**Total Progress**: 6/18 components complete (33%)

**Next Milestone**: Pillar 1 - RFM Segmentation
